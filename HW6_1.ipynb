{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4042c694-6ee3-4950-8030-731d3a1c6162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Using cached torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Using cached torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff3f37d-1dd9-4492-95bd-6b12d25f5e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "=== Processing ViT Configurations ===\n",
      "\n",
      "--- Analyzing and Training ViT_P4_E256_L4_H2_M2 ---\n",
      "Parameters: 2.16 M\n",
      "Estimated FLOPs (GMACs): 0.12 G\n",
      "\n",
      "--- Training ViT_P4_E256_L4_H2_M2 ---\n",
      "Epoch 1/10, Train Loss: 4.3519, Test Loss: 4.1155, Test Acc: 7.02%, LR: 0.000976, Time: 9.14s\n",
      "Epoch 2/10, Train Loss: 4.2415, Test Loss: 4.0648, Test Acc: 7.83%, LR: 0.000905, Time: 8.96s\n",
      "Epoch 3/10, Train Loss: 4.1865, Test Loss: 3.9802, Test Acc: 9.34%, LR: 0.000794, Time: 9.11s\n",
      "Epoch 4/10, Train Loss: 4.1616, Test Loss: 3.9669, Test Acc: 9.13%, LR: 0.000655, Time: 8.81s\n",
      "Epoch 5/10, Train Loss: 4.1416, Test Loss: 3.9366, Test Acc: 9.89%, LR: 0.000500, Time: 8.82s\n",
      "Epoch 6/10, Train Loss: 4.1247, Test Loss: 3.9048, Test Acc: 10.51%, LR: 0.000345, Time: 8.81s\n",
      "Epoch 7/10, Train Loss: 4.1013, Test Loss: 3.8826, Test Acc: 10.74%, LR: 0.000206, Time: 8.86s\n",
      "Epoch 8/10, Train Loss: 4.0872, Test Loss: 3.8828, Test Acc: 11.08%, LR: 0.000095, Time: 8.84s\n",
      "Epoch 9/10, Train Loss: 4.0761, Test Loss: 3.8631, Test Acc: 11.42%, LR: 0.000024, Time: 8.88s\n",
      "Epoch 10/10, Train Loss: 4.0664, Test Loss: 3.8531, Test Acc: 11.77%, LR: 0.000000, Time: 8.87s\n",
      "Finished Training ViT_P4_E256_L4_H2_M2. Best Test Accuracy: 11.77%\n",
      "Final Epoch Test Accuracy: 11.77%\n",
      "\n",
      "--- Analyzing and Training ViT_P4_E512_L4_H4_M4 ---\n",
      "Parameters: 12.72 M\n",
      "Estimated FLOPs (GMACs): 0.64 G\n",
      "\n",
      "--- Training ViT_P4_E512_L4_H4_M4 ---\n",
      "Epoch 1/10, Train Loss: 4.3688, Test Loss: 4.0955, Test Acc: 7.08%, LR: 0.000976, Time: 27.14s\n",
      "Epoch 2/10, Train Loss: 4.2104, Test Loss: 4.0196, Test Acc: 7.90%, LR: 0.000905, Time: 27.30s\n",
      "Epoch 3/10, Train Loss: 4.1535, Test Loss: 3.9381, Test Acc: 9.13%, LR: 0.000794, Time: 27.32s\n",
      "Epoch 4/10, Train Loss: 4.1008, Test Loss: 3.8679, Test Acc: 10.03%, LR: 0.000655, Time: 27.29s\n",
      "Epoch 5/10, Train Loss: 4.0694, Test Loss: 3.8316, Test Acc: 11.50%, LR: 0.000500, Time: 27.24s\n",
      "Epoch 6/10, Train Loss: 4.0358, Test Loss: 3.8124, Test Acc: 12.02%, LR: 0.000345, Time: 27.04s\n",
      "Epoch 7/10, Train Loss: 4.0151, Test Loss: 3.7564, Test Acc: 12.75%, LR: 0.000206, Time: 26.99s\n",
      "Epoch 8/10, Train Loss: 3.9972, Test Loss: 3.7389, Test Acc: 12.91%, LR: 0.000095, Time: 27.03s\n",
      "Epoch 9/10, Train Loss: 3.9716, Test Loss: 3.7133, Test Acc: 13.43%, LR: 0.000024, Time: 26.94s\n",
      "Epoch 10/10, Train Loss: 3.9671, Test Loss: 3.7119, Test Acc: 13.45%, LR: 0.000000, Time: 26.90s\n",
      "Finished Training ViT_P4_E512_L4_H4_M4. Best Test Accuracy: 13.45%\n",
      "Final Epoch Test Accuracy: 13.45%\n",
      "\n",
      "--- Analyzing and Training ViT_P8_E256_L8_H4_M2 ---\n",
      "Parameters: 4.30 M\n",
      "Estimated FLOPs (GMACs): 0.19 G\n",
      "\n",
      "--- Training ViT_P8_E256_L8_H4_M2 ---\n",
      "Epoch 1/10, Train Loss: 4.4135, Test Loss: 4.2043, Test Acc: 5.44%, LR: 0.000976, Time: 10.34s\n",
      "Epoch 2/10, Train Loss: 4.3045, Test Loss: 4.1501, Test Acc: 6.93%, LR: 0.000905, Time: 10.48s\n",
      "Epoch 3/10, Train Loss: 4.2597, Test Loss: 4.0713, Test Acc: 7.49%, LR: 0.000794, Time: 10.56s\n",
      "Epoch 4/10, Train Loss: 4.2261, Test Loss: 4.0597, Test Acc: 7.41%, LR: 0.000655, Time: 11.16s\n",
      "Epoch 5/10, Train Loss: 4.2073, Test Loss: 4.0386, Test Acc: 7.33%, LR: 0.000500, Time: 10.14s\n",
      "Epoch 6/10, Train Loss: 4.1874, Test Loss: 3.9856, Test Acc: 8.88%, LR: 0.000345, Time: 10.30s\n",
      "Epoch 7/10, Train Loss: 4.1700, Test Loss: 3.9654, Test Acc: 9.00%, LR: 0.000206, Time: 10.36s\n",
      "Epoch 8/10, Train Loss: 4.1590, Test Loss: 3.9595, Test Acc: 9.71%, LR: 0.000095, Time: 10.59s\n",
      "Epoch 9/10, Train Loss: 4.1382, Test Loss: 3.9495, Test Acc: 9.70%, LR: 0.000024, Time: 10.77s\n",
      "Epoch 10/10, Train Loss: 4.1339, Test Loss: 3.9440, Test Acc: 9.63%, LR: 0.000000, Time: 10.33s\n",
      "Finished Training ViT_P8_E256_L8_H4_M2. Best Test Accuracy: 9.71%\n",
      "Final Epoch Test Accuracy: 9.63%\n",
      "\n",
      "--- Analyzing and Training ViT_P8_E512_L8_H4_M4 ---\n",
      "Parameters: 25.38 M\n",
      "Estimated FLOPs (GMACs): 1.18 G\n",
      "\n",
      "--- Training ViT_P8_E512_L8_H4_M4 ---\n",
      "Epoch 1/10, Train Loss: 4.4783, Test Loss: 4.2532, Test Acc: 6.06%, LR: 0.000976, Time: 19.04s\n",
      "Epoch 2/10, Train Loss: 4.3236, Test Loss: 4.1455, Test Acc: 6.91%, LR: 0.000905, Time: 19.13s\n",
      "Epoch 3/10, Train Loss: 4.2668, Test Loss: 4.0991, Test Acc: 7.31%, LR: 0.000794, Time: 19.15s\n",
      "Epoch 4/10, Train Loss: 4.2397, Test Loss: 4.0748, Test Acc: 6.64%, LR: 0.000655, Time: 19.05s\n",
      "Epoch 5/10, Train Loss: 4.2189, Test Loss: 4.0675, Test Acc: 7.48%, LR: 0.000500, Time: 19.10s\n",
      "Epoch 6/10, Train Loss: 4.2029, Test Loss: 4.0275, Test Acc: 7.65%, LR: 0.000345, Time: 19.03s\n",
      "Epoch 7/10, Train Loss: 4.1857, Test Loss: 3.9924, Test Acc: 8.46%, LR: 0.000206, Time: 18.98s\n",
      "Epoch 8/10, Train Loss: 4.1634, Test Loss: 3.9704, Test Acc: 9.01%, LR: 0.000095, Time: 18.89s\n",
      "Epoch 9/10, Train Loss: 4.1453, Test Loss: 3.9474, Test Acc: 9.44%, LR: 0.000024, Time: 18.88s\n",
      "Epoch 10/10, Train Loss: 4.1314, Test Loss: 3.9483, Test Acc: 9.35%, LR: 0.000000, Time: 18.77s\n",
      "Finished Training ViT_P8_E512_L8_H4_M4. Best Test Accuracy: 9.44%\n",
      "Final Epoch Test Accuracy: 9.35%\n",
      "\n",
      "--- Analyzing and Training ViT_P4_E256_L8_H4_M4 ---\n",
      "Parameters: 6.37 M\n",
      "Estimated FLOPs (GMACs): 0.32 G\n",
      "\n",
      "--- Training ViT_P4_E256_L8_H4_M4 ---\n",
      "Epoch 1/10, Train Loss: 4.3465, Test Loss: 4.0958, Test Acc: 6.70%, LR: 0.000976, Time: 21.48s\n",
      "Epoch 2/10, Train Loss: 4.2130, Test Loss: 4.0563, Test Acc: 7.66%, LR: 0.000905, Time: 21.52s\n",
      "Epoch 3/10, Train Loss: 4.1600, Test Loss: 3.9397, Test Acc: 9.65%, LR: 0.000794, Time: 21.42s\n",
      "Epoch 4/10, Train Loss: 4.1203, Test Loss: 3.8979, Test Acc: 10.60%, LR: 0.000655, Time: 21.45s\n",
      "Epoch 5/10, Train Loss: 4.0907, Test Loss: 3.8455, Test Acc: 10.83%, LR: 0.000500, Time: 21.38s\n",
      "Epoch 6/10, Train Loss: 4.0549, Test Loss: 3.8236, Test Acc: 11.12%, LR: 0.000345, Time: 21.29s\n",
      "Epoch 7/10, Train Loss: 4.0332, Test Loss: 3.7900, Test Acc: 12.01%, LR: 0.000206, Time: 21.27s\n",
      "Epoch 8/10, Train Loss: 4.0105, Test Loss: 3.7887, Test Acc: 12.63%, LR: 0.000095, Time: 21.22s\n",
      "Epoch 9/10, Train Loss: 3.9957, Test Loss: 3.7609, Test Acc: 13.26%, LR: 0.000024, Time: 21.23s\n",
      "Epoch 10/10, Train Loss: 3.9872, Test Loss: 3.7560, Test Acc: 13.34%, LR: 0.000000, Time: 21.26s\n",
      "Finished Training ViT_P4_E256_L8_H4_M4. Best Test Accuracy: 13.34%\n",
      "Final Epoch Test Accuracy: 13.34%\n",
      "\n",
      "--- Analyzing and Training ViT_P8_E512_L4_H2_M2 ---\n",
      "Parameters: 8.57 M\n",
      "Estimated FLOPs (GMACs): 0.37 G\n",
      "\n",
      "--- Training ViT_P8_E512_L4_H2_M2 ---\n",
      "Epoch 1/10, Train Loss: 4.4717, Test Loss: 4.2965, Test Acc: 4.98%, LR: 0.000976, Time: 8.44s\n",
      "Epoch 2/10, Train Loss: 4.3283, Test Loss: 4.1661, Test Acc: 6.47%, LR: 0.000905, Time: 8.60s\n",
      "Epoch 3/10, Train Loss: 4.2738, Test Loss: 4.0855, Test Acc: 7.29%, LR: 0.000794, Time: 8.65s\n",
      "Epoch 4/10, Train Loss: 4.2366, Test Loss: 4.0489, Test Acc: 8.04%, LR: 0.000655, Time: 8.32s\n",
      "Epoch 5/10, Train Loss: 4.2213, Test Loss: 4.0250, Test Acc: 8.32%, LR: 0.000500, Time: 8.40s\n",
      "Epoch 6/10, Train Loss: 4.2056, Test Loss: 3.9999, Test Acc: 8.84%, LR: 0.000345, Time: 8.37s\n",
      "Epoch 7/10, Train Loss: 4.1904, Test Loss: 3.9855, Test Acc: 9.66%, LR: 0.000206, Time: 8.35s\n",
      "Epoch 8/10, Train Loss: 4.1770, Test Loss: 3.9858, Test Acc: 9.21%, LR: 0.000095, Time: 8.33s\n",
      "Epoch 9/10, Train Loss: 4.1658, Test Loss: 3.9669, Test Acc: 9.43%, LR: 0.000024, Time: 8.37s\n",
      "Epoch 10/10, Train Loss: 4.1567, Test Loss: 3.9690, Test Acc: 9.21%, LR: 0.000000, Time: 8.51s\n",
      "Finished Training ViT_P8_E512_L4_H2_M2. Best Test Accuracy: 9.66%\n",
      "Final Epoch Test Accuracy: 9.21%\n",
      "\n",
      "=== Processing ResNet-18 Baseline ===\n",
      "\n",
      "--- Analyzing and Training ResNet-18 ---\n",
      "Parameters: 11.22 M\n",
      "Estimated FLOPs (GMACs): 35.55 G\n",
      "\n",
      "--- Training ResNet-18 ---\n",
      "Epoch 1/10, Train Loss: 4.3545, Test Loss: 4.2043, Test Acc: 4.86%, LR: 0.000976, Time: 13.05s\n",
      "Epoch 2/10, Train Loss: 4.2386, Test Loss: 4.1340, Test Acc: 5.22%, LR: 0.000905, Time: 12.98s\n",
      "Epoch 3/10, Train Loss: 4.1842, Test Loss: 3.9999, Test Acc: 7.74%, LR: 0.000794, Time: 12.94s\n",
      "Epoch 4/10, Train Loss: 4.0992, Test Loss: 3.8917, Test Acc: 8.31%, LR: 0.000655, Time: 12.96s\n",
      "Epoch 5/10, Train Loss: 4.0179, Test Loss: 3.7649, Test Acc: 10.25%, LR: 0.000500, Time: 12.97s\n",
      "Epoch 6/10, Train Loss: 3.9387, Test Loss: 3.6656, Test Acc: 11.89%, LR: 0.000345, Time: 12.86s\n",
      "Epoch 7/10, Train Loss: 3.8545, Test Loss: 3.5306, Test Acc: 13.80%, LR: 0.000206, Time: 23.74s\n",
      "Epoch 8/10, Train Loss: 3.7707, Test Loss: 3.4528, Test Acc: 15.08%, LR: 0.000095, Time: 26.33s\n",
      "Epoch 9/10, Train Loss: 3.7177, Test Loss: 3.3666, Test Acc: 17.81%, LR: 0.000024, Time: 26.27s\n",
      "Epoch 10/10, Train Loss: 3.6737, Test Loss: 3.3136, Test Acc: 18.20%, LR: 0.000000, Time: 26.31s\n",
      "Finished Training ResNet-18. Best Test Accuracy: 18.20%\n",
      "Final Epoch Test Accuracy: 18.20%\n",
      "\n",
      "--- Results Summary ---\n",
      "    Model Configuration Params (M) FLOPs (GMACs) Avg Epoch Time (s) Test Acc (%) @10 epochs\n",
      "0  ViT_P4_E256_L4_H2_M2       2.16          0.12               8.91                   11.77\n",
      "1  ViT_P4_E512_L4_H4_M4      12.72          0.64              27.12                   13.45\n",
      "2  ViT_P8_E256_L8_H4_M2       4.30          0.19              10.50                    9.71\n",
      "3  ViT_P8_E512_L8_H4_M4      25.38          1.18              19.00                    9.44\n",
      "4  ViT_P4_E256_L8_H4_M4       6.37          0.32              21.35                   13.34\n",
      "5  ViT_P8_E512_L4_H2_M2       8.57          0.37               8.43                    9.66\n",
      "6             ResNet-18      11.22         35.55              18.04                   18.20\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ViT from Scratch for CIFAR-100 Classification and Comparison with ResNet-18.\n",
    "Based on user-provided sample, extended for multiple configurations and comparison.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary # For FLOPs/Params estimation\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd # For better table display\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define configurations to test\n",
    "# Format: (config_name, patch_size, embed_dim, num_layers, num_heads, mlp_ratio)\n",
    "vit_configs = [\n",
    "    (\"ViT_P4_E256_L4_H2_M2\", 4, 256, 4, 2, 2.0),\n",
    "    (\"ViT_P4_E512_L4_H4_M4\", 4, 512, 4, 4, 4.0), # Example with 4x embed_dim for MLP\n",
    "    (\"ViT_P8_E256_L8_H4_M2\", 8, 256, 8, 4, 2.0),\n",
    "    (\"ViT_P8_E512_L8_H4_M4\", 8, 512, 8, 4, 4.0), # Example with 4x embed_dim for MLP\n",
    "    # Add your required configurations from the prompt:\n",
    "    (\"ViT_P4_E256_L8_H4_M4\", 4, 256, 8, 4, 4.0),\n",
    "    (\"ViT_P8_E512_L4_H2_M2\", 8, 512, 4, 2, 2.0),\n",
    "]\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 64 # From prompt\n",
    "EPOCHS = 10     # Train for 10 epochs for baseline comparison as requested\n",
    "LR = 0.001      # From prompt\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLASSES = 100 # For CIFAR-100\n",
    "IMG_SIZE = 32     # For CIFAR-100\n",
    "IN_CHANNELS = 3\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "# Using standard CIFAR-100 stats for normalization\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)), # Ensure correct size\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.TrivialAugmentWide(), # Add more augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)), # Ensure correct size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "# Use CIFAR100 as requested\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# --- Vision Transformer Implementation (Based on Sample) ---\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split image into patches and embed them.\"\"\"\n",
    "    def __init__(self, image_size, patch_size, in_channels=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, embed_dim, H', W']\n",
    "        x = x.flatten(2)  # [B, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # [B, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "# Using nn.MultiheadAttention requires careful handling of shapes (or batch_first=True)\n",
    "# Adjusted TransformerEncoder based on sample but using standard blocks\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block (using Pre-LN).\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        # Ensure batch_first=True matches input shape (B, N, E)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout) # Add dropout layer if needed outside attention/mlp\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-LN structure\n",
    "        residual = x\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output, _ = self.attention(x_norm, x_norm, x_norm) # Self-attention\n",
    "        x = residual + self.dropout(attn_output) # Apply dropout after attention\n",
    "\n",
    "        residual = x\n",
    "        x_norm = self.norm2(x)\n",
    "        mlp_output = self.mlp(x_norm)\n",
    "        x = residual + mlp_output # MLP dropout is inside the sequential\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer main model (based on sample).\"\"\"\n",
    "    def __init__(self, image_size, patch_size, num_classes, embed_dim,\n",
    "                 num_heads, num_layers, mlp_ratio, dropout=0.1): # Added mlp_ratio\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(image_size, patch_size, IN_CHANNELS, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_dropout = nn.Dropout(dropout) # Use pos_dropout for positional embedding\n",
    "\n",
    "        # Calculate mlp_dim based on embed_dim and mlp_ratio\n",
    "        mlp_dim_calculated = int(embed_dim * mlp_ratio)\n",
    "\n",
    "        # Corrected Transformer blocks using ModuleList\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim, num_heads, mlp_dim_calculated, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim) # Final LayerNorm\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Weight Initialization\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights_fn)\n",
    "\n",
    "    def _init_weights_fn(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x) # Apply dropout here\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x) # Apply final norm\n",
    "        cls_token_final = x[:, 0] # Get CLS token output\n",
    "        x = self.head(cls_token_final)\n",
    "        return x\n",
    "\n",
    "# --- ResNet-18 Baseline ---\n",
    "# Using torchvision's ResNet-18, modified for CIFAR\n",
    "resnet18_model = torchvision.models.resnet18(weights=None, num_classes=NUM_CLASSES) # Use weights=None for training from scratch\n",
    "# Adjust first conv layer and remove maxpool for CIFAR-100\n",
    "resnet18_model.conv1 = nn.Conv2d(IN_CHANNELS, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "resnet18_model.maxpool = nn.Identity()\n",
    "\n",
    "# --- Training and Evaluation Loop ---\n",
    "def train_model(model, model_name, trainloader, testloader, criterion, optimizer, epochs, device):\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    model.to(device)\n",
    "    # Use a scheduler for potentially better results, especially for ViT\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    results = {'train_loss': [], 'test_loss': [], 'test_acc': [], 'epoch_time': []}\n",
    "    best_acc = 0.0\n",
    "    # model_checkpoint = copy.deepcopy(model.state_dict()) # Keep track of best model\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # Optional: Print batch loss\n",
    "            # if (i+1) % 100 == 0:\n",
    "            #    print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(trainloader)}], Batch Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        results['train_loss'].append(epoch_loss)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_test_loss = test_loss / len(testloader)\n",
    "        epoch_test_acc = 100 * correct / total\n",
    "        results['test_loss'].append(epoch_test_loss)\n",
    "        results['test_acc'].append(epoch_test_acc)\n",
    "\n",
    "        # Update best accuracy\n",
    "        if epoch_test_acc > best_acc:\n",
    "             best_acc = epoch_test_acc\n",
    "             # model_checkpoint = copy.deepcopy(model.state_dict()) # Save best weights\n",
    "\n",
    "        scheduler.step() # Step the scheduler\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_duration = end_time - start_time\n",
    "        results['epoch_time'].append(epoch_duration)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.2f}%, LR: {optimizer.param_groups[0]['lr']:.6f}, Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "    print(f\"Finished Training {model_name}. Best Test Accuracy: {best_acc:.2f}%\")\n",
    "    avg_epoch_time = sum(results['epoch_time']) / len(results['epoch_time']) if results['epoch_time'] else 0\n",
    "    # Return final epoch accuracy or best accuracy - returning best as requested by prompt analysis\n",
    "    final_acc = results['test_acc'][-1] if results['test_acc'] else 0\n",
    "    print(f\"Final Epoch Test Accuracy: {final_acc:.2f}%\")\n",
    "    return best_acc, avg_epoch_time # Return best accuracy achieved over the epochs\n",
    "\n",
    "# --- Model Analysis and Training Execution ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "results_data = [] # List to store results for pandas DataFrame\n",
    "\n",
    "# Analyze and Train ViT Configurations\n",
    "print(\"\\n=== Processing ViT Configurations ===\")\n",
    "for config in vit_configs:\n",
    "    config_name, patch_size, embed_dim, num_layers, num_heads, mlp_ratio = config\n",
    "    print(f\"\\n--- Analyzing and Training {config_name} ---\")\n",
    "\n",
    "    # Check if patch size is valid for image size\n",
    "    if IMG_SIZE % patch_size != 0:\n",
    "        print(f\"Skipping config {config_name}: Image size {IMG_SIZE} not divisible by patch size {patch_size}\")\n",
    "        results_data.append({\n",
    "            \"Model Configuration\": config_name,\n",
    "            \"Params (M)\": \"N/A\", \"FLOPs (GMACs)\": \"N/A\",\n",
    "            \"Avg Epoch Time (s)\": \"N/A\", f\"Test Acc (%) @{EPOCHS} epochs\": \"N/A (Skipped)\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    vit_model = VisionTransformer(\n",
    "        image_size=IMG_SIZE,\n",
    "        patch_size=patch_size,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        embed_dim=embed_dim,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        mlp_ratio=mlp_ratio,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Get model summary (Params, FLOPs) using torchinfo\n",
    "    try:\n",
    "        model_stats = summary(vit_model, input_size=(BATCH_SIZE, IN_CHANNELS, IMG_SIZE, IMG_SIZE), verbose=0)\n",
    "        num_params = model_stats.total_params\n",
    "        flops = model_stats.total_mult_adds / 1e9 # GigaMACs\n",
    "        params_m = f\"{num_params/1e6:.2f}\"\n",
    "        flops_g = f\"{flops:.2f}\"\n",
    "        print(f\"Parameters: {params_m} M\")\n",
    "        print(f\"Estimated FLOPs (GMACs): {flops_g} G\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get summary for {config_name}: {e}\")\n",
    "        num_params = -1\n",
    "        flops = -1\n",
    "        params_m = \"Error\"\n",
    "        flops_g = \"Error\"\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    optimizer = optim.Adam(vit_model.parameters(), lr=LR, weight_decay=0.01) # Added weight decay\n",
    "    best_acc, avg_epoch_time = train_model(vit_model, config_name, train_loader, test_loader, criterion, optimizer, EPOCHS, DEVICE)\n",
    "\n",
    "    results_data.append({\n",
    "        \"Model Configuration\": config_name,\n",
    "        \"Params (M)\": params_m,\n",
    "        \"FLOPs (GMACs)\": flops_g,\n",
    "        \"Avg Epoch Time (s)\": f\"{avg_epoch_time:.2f}\",\n",
    "        f\"Test Acc (%) @{EPOCHS} epochs\": f\"{best_acc:.2f}\"\n",
    "    })\n",
    "    del vit_model # Free up memory\n",
    "    if DEVICE == 'cuda': torch.cuda.empty_cache() # Clear CUDA cache\n",
    "\n",
    "\n",
    "# Analyze and Train ResNet-18 Baseline\n",
    "print(\"\\n=== Processing ResNet-18 Baseline ===\")\n",
    "print(\"\\n--- Analyzing and Training ResNet-18 ---\")\n",
    "resnet18_model_instance = copy.deepcopy(resnet18_model).to(DEVICE) # Use deepcopy to avoid modifying original\n",
    "try:\n",
    "    model_stats_resnet = summary(resnet18_model_instance, input_size=(BATCH_SIZE, IN_CHANNELS, IMG_SIZE, IMG_SIZE), verbose=0)\n",
    "    num_params_resnet = model_stats_resnet.total_params\n",
    "    flops_resnet = model_stats_resnet.total_mult_adds / 1e9 # GigaMACs\n",
    "    params_m_resnet = f\"{num_params_resnet/1e6:.2f}\"\n",
    "    flops_g_resnet = f\"{flops_resnet:.2f}\"\n",
    "    print(f\"Parameters: {params_m_resnet} M\")\n",
    "    print(f\"Estimated FLOPs (GMACs): {flops_g_resnet} G\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get summary for ResNet-18: {e}\")\n",
    "    num_params_resnet = -1\n",
    "    flops_resnet = -1\n",
    "    params_m_resnet = \"Error\"\n",
    "    flops_g_resnet = \"Error\"\n",
    "\n",
    "# Train ResNet-18\n",
    "optimizer_resnet = optim.Adam(resnet18_model_instance.parameters(), lr=LR, weight_decay=0.01) # Added weight decay\n",
    "best_acc_resnet, avg_epoch_time_resnet = train_model(resnet18_model_instance, \"ResNet-18\", train_loader, test_loader, criterion, optimizer_resnet, EPOCHS, DEVICE)\n",
    "\n",
    "results_data.append({\n",
    "    \"Model Configuration\": \"ResNet-18\",\n",
    "    \"Params (M)\": params_m_resnet,\n",
    "    \"FLOPs (GMACs)\": flops_g_resnet,\n",
    "    \"Avg Epoch Time (s)\": f\"{avg_epoch_time_resnet:.2f}\",\n",
    "    f\"Test Acc (%) @{EPOCHS} epochs\": f\"{best_acc_resnet:.2f}\"\n",
    "})\n",
    "\n",
    "# --- Display Summary Table ---\n",
    "print(\"\\n--- Results Summary ---\")\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12431165-ef6b-429a-81a8-c5fb43b6b463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Jim1)",
   "language": "python",
   "name": "jim1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
