{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85292ed6-09f1-4f00-9eac-b47ac7e2a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjim/anaconda3/envs/Jim1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:02<00:00, 83.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SwinForImageClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([1000, 768]) from checkpoint, the shape in current model is torch.Size([100, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([100]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Fine-tune Swin-Tiny and Swin-Small\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m fine_tune_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/swin-tiny-patch4-window7-224\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     79\u001b[0m fine_tune_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/swin-small-patch4-window7-224\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mfine_tune_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfine_tune_model\u001b[39m(model_name):\n\u001b[0;32m---> 33\u001b[0m     model \u001b[38;5;241m=\u001b[39m SwinForImageClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, num_labels\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m     34\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Freeze the backbone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Jim1/lib/python3.12/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/Jim1/lib/python3.12/site-packages/transformers/modeling_utils.py:4319\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4312\u001b[0m     (\n\u001b[1;32m   4313\u001b[0m         model,\n\u001b[1;32m   4314\u001b[0m         missing_keys,\n\u001b[1;32m   4315\u001b[0m         unexpected_keys,\n\u001b[1;32m   4316\u001b[0m         mismatched_keys,\n\u001b[1;32m   4317\u001b[0m         offload_index,\n\u001b[1;32m   4318\u001b[0m         error_msgs,\n\u001b[0;32m-> 4319\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   4320\u001b[0m         model,\n\u001b[1;32m   4321\u001b[0m         state_dict,\n\u001b[1;32m   4322\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   4323\u001b[0m         resolved_archive_file,\n\u001b[1;32m   4324\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   4325\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   4326\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   4327\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   4328\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   4329\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4330\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4331\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   4332\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   4333\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   4334\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4335\u001b[0m         gguf_path\u001b[38;5;241m=\u001b[39mgguf_path,\n\u001b[1;32m   4336\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[1;32m   4337\u001b[0m     )\n\u001b[1;32m   4339\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4340\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/anaconda3/envs/Jim1/lib/python3.12/site-packages/transformers/modeling_utils.py:4955\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_msg:\n\u001b[1;32m   4952\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4953\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4954\u001b[0m         )\n\u001b[0;32m-> 4955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4958\u001b[0m     archs \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SwinForImageClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([1000, 768]) from checkpoint, the shape in current model is torch.Size([100, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([100]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "from transformers import SwinForImageClassification, SwinConfig\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 2e-5\n",
    "num_classes = 100\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                              download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                             download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load pretrained models\n",
    "def fine_tune_model(model_name):\n",
    "    model = SwinForImageClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Freeze the backbone\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace classification head\n",
    "    model.classifier = torch.nn.Linear(model.classifier.in_features, num_classes)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Fine-tune Swin-Tiny and Swin-Small\n",
    "fine_tune_model('microsoft/swin-tiny-patch4-window7-224')\n",
    "fine_tune_model('microsoft/swin-small-patch4-window7-224')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71ce978e-f986-41bb-9927-edbf23d4b4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/5], Step [100], Loss: 4.6048\n",
      "Epoch [1/5], Step [200], Loss: 4.5395\n",
      "Epoch [1/5], Step [300], Loss: 4.4670\n",
      "Epoch [1/5], Step [400], Loss: 4.3737\n",
      "Epoch [1/5], Step [500], Loss: 4.4301\n",
      "Epoch [1/5], Step [600], Loss: 4.3668\n",
      "Epoch [1/5], Step [700], Loss: 4.2453\n",
      "Epoch [1/5], Step [800], Loss: 4.2405\n",
      "Epoch [1/5], Step [900], Loss: 4.2085\n",
      "Epoch [1/5], Step [1000], Loss: 4.2174\n",
      "Epoch [1/5], Step [1100], Loss: 4.1223\n",
      "Epoch [1/5], Step [1200], Loss: 4.0029\n",
      "Epoch [1/5], Step [1300], Loss: 4.0702\n",
      "Epoch [1/5], Step [1400], Loss: 4.0339\n",
      "Epoch [1/5], Step [1500], Loss: 3.8960\n",
      "Epoch [2/5], Step [100], Loss: 3.8272\n",
      "Epoch [2/5], Step [200], Loss: 3.7390\n",
      "Epoch [2/5], Step [300], Loss: 3.8402\n",
      "Epoch [2/5], Step [400], Loss: 3.6750\n",
      "Epoch [2/5], Step [500], Loss: 3.6564\n",
      "Epoch [2/5], Step [600], Loss: 3.6063\n",
      "Epoch [2/5], Step [700], Loss: 3.6388\n",
      "Epoch [2/5], Step [800], Loss: 3.6578\n",
      "Epoch [2/5], Step [900], Loss: 3.5634\n",
      "Epoch [2/5], Step [1000], Loss: 3.4862\n",
      "Epoch [2/5], Step [1100], Loss: 3.5264\n",
      "Epoch [2/5], Step [1200], Loss: 3.5622\n",
      "Epoch [2/5], Step [1300], Loss: 3.4094\n",
      "Epoch [2/5], Step [1400], Loss: 3.3732\n",
      "Epoch [2/5], Step [1500], Loss: 3.3030\n",
      "Epoch [3/5], Step [100], Loss: 3.1718\n",
      "Epoch [3/5], Step [200], Loss: 3.2291\n",
      "Epoch [3/5], Step [300], Loss: 3.4455\n",
      "Epoch [3/5], Step [400], Loss: 2.9815\n",
      "Epoch [3/5], Step [500], Loss: 3.1392\n",
      "Epoch [3/5], Step [600], Loss: 3.2097\n",
      "Epoch [3/5], Step [700], Loss: 3.1335\n",
      "Epoch [3/5], Step [800], Loss: 3.0430\n",
      "Epoch [3/5], Step [900], Loss: 2.9604\n",
      "Epoch [3/5], Step [1000], Loss: 3.3007\n",
      "Epoch [3/5], Step [1100], Loss: 3.0390\n",
      "Epoch [3/5], Step [1200], Loss: 2.7085\n",
      "Epoch [3/5], Step [1300], Loss: 2.9859\n",
      "Epoch [3/5], Step [1400], Loss: 2.9148\n",
      "Epoch [3/5], Step [1500], Loss: 2.8007\n",
      "Epoch [4/5], Step [100], Loss: 2.9000\n",
      "Epoch [4/5], Step [200], Loss: 2.5912\n",
      "Epoch [4/5], Step [300], Loss: 2.7492\n",
      "Epoch [4/5], Step [400], Loss: 2.8081\n",
      "Epoch [4/5], Step [500], Loss: 2.6057\n",
      "Epoch [4/5], Step [600], Loss: 2.7118\n",
      "Epoch [4/5], Step [700], Loss: 2.7589\n",
      "Epoch [4/5], Step [800], Loss: 2.4081\n",
      "Epoch [4/5], Step [900], Loss: 2.5695\n",
      "Epoch [4/5], Step [1000], Loss: 2.3748\n",
      "Epoch [4/5], Step [1100], Loss: 2.5867\n",
      "Epoch [4/5], Step [1200], Loss: 2.6837\n",
      "Epoch [4/5], Step [1300], Loss: 2.3482\n",
      "Epoch [4/5], Step [1400], Loss: 2.7647\n",
      "Epoch [4/5], Step [1500], Loss: 2.2973\n",
      "Epoch [5/5], Step [100], Loss: 2.0743\n",
      "Epoch [5/5], Step [200], Loss: 2.3719\n",
      "Epoch [5/5], Step [300], Loss: 2.6768\n",
      "Epoch [5/5], Step [400], Loss: 2.2282\n",
      "Epoch [5/5], Step [500], Loss: 2.4635\n",
      "Epoch [5/5], Step [600], Loss: 1.9277\n",
      "Epoch [5/5], Step [700], Loss: 2.2889\n",
      "Epoch [5/5], Step [800], Loss: 2.0077\n",
      "Epoch [5/5], Step [900], Loss: 2.3369\n",
      "Epoch [5/5], Step [1000], Loss: 2.2250\n",
      "Epoch [5/5], Step [1100], Loss: 2.2171\n",
      "Epoch [5/5], Step [1200], Loss: 2.0995\n",
      "Epoch [5/5], Step [1300], Loss: 2.5144\n",
      "Epoch [5/5], Step [1400], Loss: 1.9395\n",
      "Epoch [5/5], Step [1500], Loss: 2.5018\n",
      "Accuracy: 61.78%\n",
      "Epoch [1/5], Step [100], Loss: 4.5667\n",
      "Epoch [1/5], Step [200], Loss: 4.5606\n",
      "Epoch [1/5], Step [300], Loss: 4.4013\n",
      "Epoch [1/5], Step [400], Loss: 4.3737\n",
      "Epoch [1/5], Step [500], Loss: 4.3767\n",
      "Epoch [1/5], Step [600], Loss: 4.3563\n",
      "Epoch [1/5], Step [700], Loss: 4.1605\n",
      "Epoch [1/5], Step [800], Loss: 4.1998\n",
      "Epoch [1/5], Step [900], Loss: 4.1513\n",
      "Epoch [1/5], Step [1000], Loss: 4.0506\n",
      "Epoch [1/5], Step [1100], Loss: 4.0275\n",
      "Epoch [1/5], Step [1200], Loss: 3.9059\n",
      "Epoch [1/5], Step [1300], Loss: 3.9312\n",
      "Epoch [1/5], Step [1400], Loss: 3.9396\n",
      "Epoch [1/5], Step [1500], Loss: 3.9645\n",
      "Epoch [2/5], Step [100], Loss: 3.7941\n",
      "Epoch [2/5], Step [200], Loss: 3.6632\n",
      "Epoch [2/5], Step [300], Loss: 3.7129\n",
      "Epoch [2/5], Step [400], Loss: 3.6226\n",
      "Epoch [2/5], Step [500], Loss: 3.5013\n",
      "Epoch [2/5], Step [600], Loss: 3.4222\n",
      "Epoch [2/5], Step [700], Loss: 3.6528\n",
      "Epoch [2/5], Step [800], Loss: 3.3621\n",
      "Epoch [2/5], Step [900], Loss: 3.4625\n",
      "Epoch [2/5], Step [1000], Loss: 3.5046\n",
      "Epoch [2/5], Step [1100], Loss: 3.3395\n",
      "Epoch [2/5], Step [1200], Loss: 3.1611\n",
      "Epoch [2/5], Step [1300], Loss: 3.0853\n",
      "Epoch [2/5], Step [1400], Loss: 2.9608\n",
      "Epoch [2/5], Step [1500], Loss: 3.0316\n",
      "Epoch [3/5], Step [100], Loss: 2.9739\n",
      "Epoch [3/5], Step [200], Loss: 3.0393\n",
      "Epoch [3/5], Step [300], Loss: 2.7362\n",
      "Epoch [3/5], Step [400], Loss: 2.8573\n",
      "Epoch [3/5], Step [500], Loss: 2.7373\n",
      "Epoch [3/5], Step [600], Loss: 3.0135\n",
      "Epoch [3/5], Step [700], Loss: 3.1346\n",
      "Epoch [3/5], Step [800], Loss: 2.5028\n",
      "Epoch [3/5], Step [900], Loss: 2.6009\n",
      "Epoch [3/5], Step [1000], Loss: 2.6599\n",
      "Epoch [3/5], Step [1100], Loss: 2.5893\n",
      "Epoch [3/5], Step [1200], Loss: 2.8125\n",
      "Epoch [3/5], Step [1300], Loss: 2.3381\n",
      "Epoch [3/5], Step [1400], Loss: 2.8043\n",
      "Epoch [3/5], Step [1500], Loss: 2.3403\n",
      "Epoch [4/5], Step [100], Loss: 2.6162\n",
      "Epoch [4/5], Step [200], Loss: 2.5044\n",
      "Epoch [4/5], Step [300], Loss: 2.7563\n",
      "Epoch [4/5], Step [400], Loss: 2.3733\n",
      "Epoch [4/5], Step [500], Loss: 2.3502\n",
      "Epoch [4/5], Step [600], Loss: 2.4291\n",
      "Epoch [4/5], Step [700], Loss: 2.0697\n",
      "Epoch [4/5], Step [800], Loss: 2.1110\n",
      "Epoch [4/5], Step [900], Loss: 2.5937\n",
      "Epoch [4/5], Step [1000], Loss: 2.2072\n",
      "Epoch [4/5], Step [1100], Loss: 2.2600\n",
      "Epoch [4/5], Step [1200], Loss: 2.1900\n",
      "Epoch [4/5], Step [1300], Loss: 2.3314\n",
      "Epoch [4/5], Step [1400], Loss: 2.1936\n",
      "Epoch [4/5], Step [1500], Loss: 2.2983\n",
      "Epoch [5/5], Step [100], Loss: 2.4176\n",
      "Epoch [5/5], Step [200], Loss: 2.1555\n",
      "Epoch [5/5], Step [300], Loss: 2.1127\n",
      "Epoch [5/5], Step [400], Loss: 2.1401\n",
      "Epoch [5/5], Step [500], Loss: 2.0351\n",
      "Epoch [5/5], Step [600], Loss: 1.8206\n",
      "Epoch [5/5], Step [700], Loss: 1.8618\n",
      "Epoch [5/5], Step [800], Loss: 2.0241\n",
      "Epoch [5/5], Step [900], Loss: 1.7515\n",
      "Epoch [5/5], Step [1000], Loss: 1.9993\n",
      "Epoch [5/5], Step [1100], Loss: 1.9140\n",
      "Epoch [5/5], Step [1200], Loss: 1.8683\n",
      "Epoch [5/5], Step [1300], Loss: 2.5320\n",
      "Epoch [5/5], Step [1400], Loss: 1.6367\n",
      "Epoch [5/5], Step [1500], Loss: 1.8623\n",
      "Accuracy: 65.39%\n"
     ]
    }
   ],
   "source": [
    "from transformers import SwinForImageClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 2e-5\n",
    "num_classes = 100\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                              download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                             download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load pretrained models\n",
    "def fine_tune_model(model_name):\n",
    "    # Load the pretrained model\n",
    "    model = SwinForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "    # Modify the classifier layer to match the number of classes in CIFAR-100\n",
    "    model.classifier = torch.nn.Linear(model.classifier.in_features, num_classes)\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Freeze the backbone (only train the classifier layer)\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Fine-tune Swin-Tiny and Swin-Small\n",
    "fine_tune_model('microsoft/swin-tiny-patch4-window7-224')\n",
    "fine_tune_model('microsoft/swin-small-patch4-window7-224')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfbe20fa-426a-4f89-b2d8-c01a91d0627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Pretrained Swin Models ===\n",
      "Loading model: microsoft/swin-tiny-patch4-window7-224\n",
      "Freezing backbone weights...\n",
      "Total Parameters: 27.60 M\n",
      "Trainable Parameters: 0.08 M\n",
      "\n",
      "--- Fine-tuning swin-tiny-patch4-window7-224 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 4.0444, Test Loss: 3.4833, Test Acc: 47.31%, Time: 153.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 3.0527, Test Loss: 2.6567, Test Acc: 58.63%, Time: 107.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 2.3741, Test Loss: 2.1184, Test Acc: 62.47%, Time: 108.22s\n",
      "Finished Training swin-tiny-patch4-window7-224. Best Test Accuracy: 62.47%\n",
      "Final Epoch Test Accuracy: 62.47%\n",
      "Loading model: microsoft/swin-small-patch4-window7-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-small-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing backbone weights...\n",
      "Total Parameters: 48.91 M\n",
      "Trainable Parameters: 0.08 M\n",
      "\n",
      "--- Fine-tuning swin-small-patch4-window7-224 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 3.9770, Test Loss: 3.3574, Test Acc: 53.54%, Time: 172.37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 2.8896, Test Loss: 2.4638, Test Acc: 63.23%, Time: 172.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 2.1679, Test Loss: 1.9089, Test Acc: 66.97%, Time: 172.34s\n",
      "Finished Training swin-small-patch4-window7-224. Best Test Accuracy: 66.97%\n",
      "Final Epoch Test Accuracy: 66.97%\n",
      "\n",
      "--- Results Summary (Fine-tuning vs Scratch) ---\n",
      "                       Model Configuration Avg Epoch Time (s) Test Acc (%) @3 epochs\n",
      " swin-tiny-patch4-window7-224 (Fine-tuned)             123.29                  62.47\n",
      "swin-small-patch4-window7-224 (Fine-tuned)             172.40                  66.97\n",
      "    Swin-Tiny (From Scratch - Placeholder)                N/A                    N/A\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fine-tuning pretrained Swin Transformers (Tiny and Small) on CIFAR-100\n",
    "and preparing for comparison with training from scratch. (Corrected)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import SwinForImageClassification, AutoImageProcessor\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# Models to fine-tune\n",
    "model_checkpoints = [\n",
    "    \"microsoft/swin-tiny-patch4-window7-224\",\n",
    "    \"microsoft/swin-small-patch4-window7-224\",\n",
    "]\n",
    "\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 32 # As requested\n",
    "EPOCHS = 3      # Fine-tune for 2-5 epochs (using 3 here, adjustable)\n",
    "LR = 2e-5       # As requested (AdamW usually uses smaller LR for fine-tuning)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 100 # For CIFAR-100\n",
    "\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "# Swin models often pretrained on 224x224 images. Resize CIFAR-100.\n",
    "processor = AutoImageProcessor.from_pretrained(model_checkpoints[0])\n",
    "image_mean = processor.image_mean\n",
    "image_std = processor.image_std\n",
    "size = processor.size[\"height\"] # Should be 224\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=image_mean, std=image_std)\n",
    "_transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "\n",
    "# Apply transforms to CIFAR-100\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True if DEVICE=='cuda' else False)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True if DEVICE=='cuda' else False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Loading and Modification ---\n",
    "def load_and_prepare_model(checkpoint, num_labels, freeze_backbone=True):\n",
    "    print(f\"Loading model: {checkpoint}\")\n",
    "    model = SwinForImageClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True, # Necessary because we are replacing the head\n",
    "    )\n",
    "\n",
    "\n",
    "    # Freeze backbone if required\n",
    "    if freeze_backbone:\n",
    "        print(\"Freezing backbone weights...\")\n",
    "        for param in model.swin.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Ensure the classifier head is trainable\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        print(\"Training entire model (backbone unfrozen)...\") # For scratch comparison later\n",
    "\n",
    "\n",
    "    # Print trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params/1e6:.2f} M\")\n",
    "    print(f\"Trainable Parameters: {trainable_params/1e6:.2f} M\")\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Training and Evaluation Loop ---\n",
    "def train_model(model, model_name, trainloader, testloader, optimizer, epochs, device):\n",
    "    print(f\"\\n--- Fine-tuning {model_name} ---\")\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss() # Define loss function inside\n",
    "    results = {'train_loss': [], 'test_loss': [], 'test_acc': [], 'epoch_time': []}\n",
    "    best_acc = 0.0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            # Assuming standard torchvision loader output (inputs, labels)\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Transformers models usually return a dictionary-like object\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits # Extract logits\n",
    "\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # *** CORRECTED LINE BELOW ***\n",
    "            progress_bar.set_postfix({'loss': loss.item()}) # Pass the float directly\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        results['train_loss'].append(epoch_loss)\n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        epoch_test_loss = test_loss / len(testloader)\n",
    "        epoch_test_acc = 100 * correct / total\n",
    "        results['test_loss'].append(epoch_test_loss)\n",
    "        results['test_acc'].append(epoch_test_acc)\n",
    "\n",
    "\n",
    "        if epoch_test_acc > best_acc:\n",
    "             best_acc = epoch_test_acc\n",
    "\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_duration = end_time - start_time\n",
    "        results['epoch_time'].append(epoch_duration)\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.2f}%, Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "\n",
    "    print(f\"Finished Training {model_name}. Best Test Accuracy: {best_acc:.2f}%\")\n",
    "    avg_epoch_time = sum(results['epoch_time']) / len(results['epoch_time']) if results['epoch_time'] else 0\n",
    "    final_acc = results['test_acc'][-1] if results['test_acc'] else 0 # Use final epoch accuracy for report\n",
    "    print(f\"Final Epoch Test Accuracy: {final_acc:.2f}%\")\n",
    "    return final_acc, avg_epoch_time # Return final accuracy and avg time\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "results_data = []\n",
    "\n",
    "\n",
    "print(\"\\n=== Processing Pretrained Swin Models ===\")\n",
    "for checkpoint in model_checkpoints:\n",
    "    model_name = checkpoint.split('/')[-1]\n",
    "    model = load_and_prepare_model(checkpoint, NUM_CLASSES, freeze_backbone=True)\n",
    "\n",
    "\n",
    "    # Define optimizer for fine-tuning (only optimizing the head)\n",
    "    # Use AdamW which is common for transformers\n",
    "    optimizer = optim.AdamW(model.classifier.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    final_acc, avg_epoch_time = train_model(model, model_name, train_loader, test_loader, optimizer, EPOCHS, DEVICE)\n",
    "\n",
    "\n",
    "    results_data.append({\n",
    "        \"Model Configuration\": model_name + \" (Fine-tuned)\",\n",
    "        \"Avg Epoch Time (s)\": f\"{avg_epoch_time:.2f}\",\n",
    "        f\"Test Acc (%) @{EPOCHS} epochs\": f\"{final_acc:.2f}\"\n",
    "    })\n",
    "    del model # Free up memory\n",
    "    if DEVICE == 'cuda': torch.cuda.empty_cache() # Clear CUDA cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Placeholder for Scratch Model Comparison ---\n",
    "results_data.append({\n",
    "    \"Model Configuration\": \"Swin-Tiny (From Scratch - Placeholder)\",\n",
    "    \"Avg Epoch Time (s)\": \"N/A\", # Measure if implemented\n",
    "    f\"Test Acc (%) @{EPOCHS} epochs\": \"N/A\" # Get from scratch run\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Display Summary Table ---\n",
    "print(\"\\n--- Results Summary (Fine-tuning vs Scratch) ---\")\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3950d7-851e-4f04-bb8c-a7acbcb4db40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Jim1)",
   "language": "python",
   "name": "jim1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
